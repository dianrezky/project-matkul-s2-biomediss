{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "li7DvRTBW3D6"
      },
      "source": [
        "## **Installing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "colab_type": "code",
        "id": "JAGmKzcQvETi",
        "outputId": "22960d60-7d29-4d2d-8d7a-2ddb70d87fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python_speech_features in c:\\users\\dianr\\.conda\\envs\\daily\\lib\\site-packages (0.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install python_speech_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gOD04lwoha8j"
      },
      "outputs": [],
      "source": [
        "#  os and argparse is done to read files from local folders\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import scipy.signal\n",
        "from python_speech_features import mfcc\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "# tesorflow is for the Deep learning model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense ,LSTM , TimeDistributed\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "import IPython\n",
        "import librosa\n",
        "from scipy.signal import butter, lfilter\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "colab_type": "code",
        "id": "E6WKgvf-vCf4",
        "outputId": "7d80ee36-5be4-41d4-884f-ed066289e627"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive  \u001b[39m# the sounds are stored in google drive \u001b[39;00m\n\u001b[0;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/gdrive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  # the sounds are stored in google drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "8S1jwaxpvPQO",
        "outputId": "c199afdf-c1c7-43e0-8ab8-6880ea346fb4"
      },
      "outputs": [],
      "source": [
        "# folder where files are stored\n",
        "file = \"dataset/training-a/wav/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e0hJIGbCX002"
      },
      "source": [
        "**about the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "CRrgF44PvZxe",
        "outputId": "7636aaaa-2eb2-4afb-a468-a5fa60a53a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MFCC EXTRACTION DONE !\n"
          ]
        }
      ],
      "source": [
        "#############################################################  MFCC EXTRACTION   ##########################################################\n",
        "# Variable assignment \n",
        "tt = 0\n",
        "time_steps = 450\n",
        "data_directory=os.getcwd()\n",
        "nfft = 1203 # Number of FFTs\n",
        "\n",
        "digit_directory = data_directory\n",
        "\n",
        "\n",
        "# To normalize the signal\n",
        "def normalize(v):\n",
        "    norm = np.linalg.norm(v)\n",
        "    if norm == 0: \n",
        "        return v\n",
        "    return v / norm\n",
        "    \n",
        "def process_directory():\n",
        "    mfcc_features = list()\n",
        "    for filename in [x for x in os.listdir(data_directory) if x.endswith('.wav')]:\n",
        "        # Read the .wav input file\n",
        "        filepath = os.path.join(digit_directory, filename)\n",
        "        sampling_freq, audio = wavfile.read(filepath)\n",
        "        label=\"n.wav\"\n",
        "\n",
        "\n",
        "# open the .hea file of the same filename to get the label as normal /abnormal.\n",
        "        st= data_directory +\"/\"+filename.split(\".\")[0]+\".hea\"\n",
        "        with open(st,'r') as f:\n",
        "            for line in f:\n",
        "                for word in line.split():\n",
        "                    if(word==\"Abnormal\"):\n",
        "                        label=\"a.wav\"\n",
        "\n",
        "\n",
        "# now we have the label stored in 'label' and the audio as 'audio' with sampling freq. as 'sampling_freq'.\n",
        "        #audio1 = audio[dic[filename.split(\".\")[0]]:]\n",
        "        audio1 = SVDnoise(audio/32768)\n",
        "        #audio1 = audio\n",
        "        temp = mfcc(audio1, sampling_freq, nfft=nfft)\n",
        "        temp = temp[tt:tt+time_steps,:]\n",
        "        mfcc_features.append({\"label\": label, \"mfcc\": temp })\n",
        "\n",
        "\n",
        "        # mfcc features of this audio has been appended to the list \n",
        "    return mfcc_features\n",
        "\n",
        "\n",
        "###########################   CREATING MFCC FEATURES   ############################\n",
        "processed_files = list()\n",
        "mfcc_features = process_directory()\n",
        "random.shuffle(mfcc_features)\n",
        "\n",
        "############   TRAINING DATA   ###########\n",
        "size = (8*len(mfcc_features))/10\n",
        "train_features = mfcc_features[0:int(size)]\n",
        "test_list = mfcc_features[int(size+1):]\n",
        "train_size = 0\n",
        "for feature in train_features:\n",
        "    train_size += 1\n",
        "    processed_files.append({'label': feature[\"label\"], 'feature': feature[\"mfcc\"] })\n",
        "# Train rnn for each MFCC and add to training set\n",
        "x = np.zeros((train_size, time_steps ,13))\n",
        "y_train = np.zeros((train_size))\n",
        "i = 0\n",
        "for processed_file in processed_files:\n",
        "#       print(processed_file['label'])\n",
        "#       print(processed_file['feature'].shape)\n",
        "    x[i,:,:] = processed_file['feature']\n",
        "    s = processed_file['label']\n",
        "    if(s[0]=='a'):\n",
        "        y_train[i]=1\n",
        "    else:\n",
        "        y_train[i]=0\n",
        "    i += 1\n",
        "normalize(x)\n",
        "\n",
        "############   TESTING DATA   #############\n",
        "test_files = list()\n",
        "test_features = test_list\n",
        "test_size = 0\n",
        "for feature in test_features:\n",
        "    test_size += 1\n",
        "    test_files.append({'label': feature[\"label\"], 'feature': feature[\"mfcc\"] })\n",
        "y_test = np.zeros((test_size))\n",
        "x_test = np.zeros((test_size, time_steps ,13))\n",
        "i = 0\n",
        "for test_file in test_files:\n",
        "    x_test[i,:,:] = test_file['feature']\n",
        "    s = test_file['label']\n",
        "#             print(s)\n",
        "    if(s[0]=='a'):\n",
        "        y_test[i]=1\n",
        "    else:\n",
        "        y_test[i]=0\n",
        "    i += 1\n",
        "normalize(x_test)\n",
        "\n",
        "print('MFCC EXTRACTION DONE !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ZaT_R-l04RW"
      },
      "source": [
        "## RNN MODEL\n",
        "### Model Structure:\n",
        "- visible layer or input layer : size of input = 13 (mfcc matrix has column size 13)\n",
        "- hidden layer 1: LSTM layer\n",
        "- hidden layer 2: LSTM layer\n",
        "- 1 dense layer having activation function = \"relu\" (rectified linear)\n",
        "- output layer : classification  \n",
        "\n",
        "![download.png](attachment:download.png)  \n",
        "<font size=\"2\">*In the figure, the dense layer is missing, but this is to give a brief idea of how the data is flowing in the model structure.*</font>  \n",
        "### Building the Model:\n",
        "- loss function: to compute the loss (currently \"mean squared error\")\n",
        "- optimizer function: adam\n",
        "- metrics: accuracy\n",
        "- **model.fit:** this function tries for the best possible fit of the model to the training data.\n",
        "<br>\n",
        "<font size=\"2\"> <font color=\"brown\"> The later part of the code was to try the model for different values of Dopout(lmabda) to calculate accuracy.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RmDoZ-M-vZr_"
      },
      "outputs": [],
      "source": [
        "dropouts = np.array([0.15])\n",
        "accuracy = np.zeros(len(dropouts),dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "sMiXU1q1vZlt",
        "outputId": "2e53da3a-d7fe-4bf3-d914-1eec6ef3b2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.15\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m Lambda \u001b[39m=\u001b[39m dr\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(dr)\n\u001b[1;32m---> 34\u001b[0m model\u001b[39m.\u001b[39;49mfit(x, y_train, epochs\u001b[39m=\u001b[39;49mEpoch_size, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m predict\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[0;32m     36\u001b[0m y_pred\u001b[39m=\u001b[39m(predict\u001b[39m>\u001b[39m\u001b[39m0.1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\dianr\\.conda\\envs\\daily\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\dianr\\.conda\\envs\\daily\\lib\\site-packages\\keras\\engine\\training.py:1697\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1695\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1697\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1698\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `train_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1699\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Empty logs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1700\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1701\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1704\u001b[0m     )\n\u001b[0;32m   1705\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[0;32m   1706\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
            "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ],
      "source": [
        "#########################  VARIABLES  ######################\n",
        "cell_no = 13\n",
        "Epoch_size = 100\n",
        "Lambda = 0.029  # dropout variable for regularization\n",
        "# No. of LSTM layers =2\n",
        "# Cost func. = cosh\n",
        "\n",
        "####################   MODEL STRUCTURE  ####################\n",
        "visible=Input(shape=(None,13))\n",
        "hidden11 = LSTM(cell_no,return_sequences=True)(visible)\n",
        "hidden1=Dropout(Lambda)(hidden11)\n",
        "\n",
        "hidden22 = LSTM(cell_no)(hidden1)\n",
        "hidden2=Dropout(Lambda)(hidden22)\n",
        "\n",
        "hidden33 = Dense(10, activation='relu')(hidden2)\n",
        "hidden3 = Dropout(Lambda)(hidden33)\n",
        "# hidden4 = TimeDistributed(Dense(1))\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(hidden3)\n",
        "# output=Dropout(Lambda)(output1)\n",
        "\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "##################   MODEL END ########################\n",
        "count = 0\n",
        "for dr in dropouts:\n",
        "    total=0\n",
        "    for ii in range(1):\n",
        "        Lambda = dr\n",
        "        print(dr)\n",
        "\n",
        "        model.fit(x, y_train, epochs=Epoch_size, batch_size=10,verbose=1)\n",
        "        predict=model.predict(x_test)\n",
        "        y_pred=(predict>0.1)\n",
        "        acc=model.evaluate(x_test,y_test,verbose=0)\n",
        "        # print(predict)\n",
        "\n",
        "        total += acc[1]\n",
        "        print(acc)\n",
        "    total /=1\n",
        "    accuracy[count] = total\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "Z1WTvY0dyQHi",
        "outputId": "609008ca-38dc-4f8e-9abe-1b3a932d4a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4661971830985916\n",
            "0.48095238095238096\n",
            "0.4614626939177543\n",
            "[[ 2 19]\n",
            " [ 8 52]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Print f1, precision, and recall scores\n",
        "print(precision_score(y_test, y_pred , average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred , average=\"macro\"))\n",
        "print(f1_score(y_test, y_pred , average=\"macro\"))\n",
        "confusion = confusion_matrix(y_test,y_pred,labels=[0,1])\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "9qtXvBvWHt8a",
        "outputId": "7632fc5d-c7b3-4a10-c632-054b7a19fc34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "loVX2KG6zems"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Heart Sound Classification Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
